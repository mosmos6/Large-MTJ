{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fairseq Dense LM original format to JAX conversion tool.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "!pip install omegaconf torch numpy jax termcolor"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xHrMZXMYymal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwNbvZdI5pbk",
        "outputId": "15809bbd-45d1-434a-9d9d-66c00fa62c67",
        "cellView": "form"
      },
      "source": [
        "#@title Load checkpoint\n",
        "path_to_checkpoint_folder = \"./en_dense_lm_355m\"  #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "path_to_checkpoint = os.path.join(path_to_checkpoint_folder, \"model.pt\")\n",
        "path_to_dict = os.path.join(path_to_checkpoint_folder, \"dict.txt\")\n",
        "from termcolor import colored\n",
        "\n",
        "print(\"Reading from dict.txt...\")\n",
        "indices = []\n",
        "dummy_indices = []\n",
        "with open(path_to_dict) as f:\n",
        "    for line in f:\n",
        "        index = line.split()[0]\n",
        "        if not index.startswith(\"madeupword\"):\n",
        "            indices.append(int(index))\n",
        "        else:\n",
        "            dummy_indices.append(int(index[10:]))\n",
        "mapping = [...] * (len(indices) + len(dummy_indices) + 4)\n",
        "i = 0\n",
        "for index in range(4):\n",
        "    mapping[len(indices) + index] = i\n",
        "    i += 1\n",
        "for index in indices:\n",
        "    mapping[index] = i\n",
        "    i += 1\n",
        "for index in dummy_indices:\n",
        "    mapping[len(indices) + 4 + index] = i\n",
        "    i += 1\n",
        "assert min(mapping) == 0 and max(mapping) == len(mapping) - 1 and len(set(mapping)) == len(mapping)\n",
        "import torch\n",
        "mapping = torch.tensor(mapping)\n",
        "print(f\"Computed embedding map of length {len(mapping)}.\")\n",
        "print(\"Reading from checkpoint...\")\n",
        "torch_checkpoint = torch.load(path_to_checkpoint, map_location='cpu')\n",
        "config = {\n",
        "    \"compat\": \"fairseq_lm\",\n",
        "    \"n_vocab\": 51200,\n",
        "    \"n_vocab_padding\": 0,\n",
        "    \"norm\": \"layernorm\",\n",
        "    \"pe\": \"fairseq_sinusoidal\",\n",
        "    \"seq\": 2048,\n",
        "    \"layers\": torch_checkpoint[\"cfg\"][\"model\"][\"decoder_layers\"],\n",
        "    \"d_model\": torch_checkpoint[\"cfg\"][\"model\"][\"decoder_embed_dim\"],\n",
        "    \"n_heads\": torch_checkpoint[\"cfg\"][\"model\"][\"decoder_attention_heads\"],\n",
        "}\n",
        "for c in (8, 6, 4, 2, 1):\n",
        "    if 0 == config[\"n_heads\"] % c == config[\"d_model\"] % c:\n",
        "        config[\"cores_per_replica\"] = c\n",
        "        break\n",
        "pieces = 16\n",
        "layers = config[\"layers\"]\n",
        "d_model = config[\"d_model\"]\n",
        "total_shards = config[\"cores_per_replica\"]\n",
        "config[\"n_vocab_padding\"] = padding_rows = -(config[\"n_vocab\"] % -total_shards)\n",
        "for i in range(total_shards):\n",
        "    os.makedirs(f\"jax_checkpoint/shard_{i}\")\n",
        "print(f\"Detected {layers} layers, {config['n_heads']} heads.  Embed dim {d_model}.  {total_shards} shards.  {padding_rows} embedding matrix padding rows.\")\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmEE1tcf6ADW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53420d0e-5554-4012-e29b-3d98041e33df"
      },
      "source": [
        "#@title Convert checkpoint to be JAX-compatible { display-mode: \"form\" }\n",
        "from termcolor import colored\n",
        "import torch\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import json\n",
        "\n",
        "def reshard_reverse(x, old_shape, is_shard_bias=False):\n",
        "    if len(x.shape) == 1:\n",
        "        assert False\n",
        "        out = x[0:1]\n",
        "\n",
        "    elif len(x.shape) == 2:\n",
        "        #print(f\"LN/bias\")\n",
        "        if old_shape[1] == x.shape[1]:\n",
        "            #print(\"LN\")\n",
        "            if not is_shard_bias:\n",
        "                out = np.tile(x[0:1], (total_shards, 1))\n",
        "            else:\n",
        "                #print(\"shard bias\")\n",
        "                out = np.tile(x[0:1], (total_shards, 1)) / total_shards\n",
        "        else:\n",
        "            #print(\"bias\")\n",
        "            out = x.reshape(old_shape)\n",
        "\n",
        "    elif len(x.shape) == 3:\n",
        "        if x.shape[0] * x.shape[2] == old_shape[2]:\n",
        "            #print(\"case 1\")\n",
        "            out = x.reshape(old_shape)\n",
        "        elif x.shape[0] * x.shape[1] == old_shape[1]:\n",
        "            #print(\"case 2\")\n",
        "            out = jnp.transpose(x.reshape((old_shape[1], old_shape[0], old_shape[2])), (1, 0, 2))\n",
        "        else:\n",
        "            raise Exception(f\"unimplemented, {x.shape}, {old_shape}\")\n",
        "    else:\n",
        "        raise Exception(f\"unimplemented, {x}\")\n",
        "    #flattened, structure = jax.tree_flatten(out)\n",
        "    #return flattened\n",
        "    return out\n",
        "\n",
        "def get_old_shape(t, dim=2):\n",
        "    if len(t.shape) == 2:\n",
        "        shard_shape = t.shape\n",
        "        if dim == 1:\n",
        "            assert shard_shape[0] % total_shards == 0\n",
        "            return (shard_shape[0] // total_shards, shard_shape[1])\n",
        "        elif dim == 2:\n",
        "            assert shard_shape[1] % total_shards == 0\n",
        "            return (shard_shape[0], shard_shape[1] // total_shards)\n",
        "        else:\n",
        "            raise ValueError(f\"unsupported dim {dim}\")\n",
        "    if len(t.shape) == 1:\n",
        "        assert t.shape[0] % total_shards == 0\n",
        "        return (t.shape[0] // total_shards,)\n",
        "    else:\n",
        "        raise ValueError(f\"unsupported shape {t.shape}\")\n",
        "\n",
        "\n",
        "def split(a, n):\n",
        "    k, m = divmod(len(a), n)\n",
        "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
        "\n",
        "def save(cpu_flattened):\n",
        "    for i in range(total_shards):\n",
        "        cpu_flattened_chunked = split(cpu_flattened, pieces)\n",
        "        for j, chunk in enumerate(cpu_flattened_chunked):\n",
        "            with open(f\"jax_checkpoint/shard_{i}/{j}.npz\", \"wb\") as f:\n",
        "                np.savez(f, *map(lambda c: c[i], chunk))\n",
        "\n",
        "\n",
        "transforms = [\n",
        "    (\"decoder.embed_tokens.weight\", False, 1)\n",
        "]\n",
        "\n",
        "checkpoint = []\n",
        "\n",
        "layer_names = sorted(map(str, range(layers)))\n",
        "for layer in layer_names:\n",
        "    transforms.extend([\n",
        "        (f\"decoder.layers.{layer}.self_attn.q_proj.bias\", False, 1),\n",
        "        (f\"decoder.layers.{layer}.self_attn.q_proj.weight\", False, 2),\n",
        "        (f\"decoder.layers.{layer}.self_attn.v_proj.bias\", False, 1),\n",
        "        (f\"decoder.layers.{layer}.self_attn.v_proj.weight\", False, 2),\n",
        "        (f\"decoder.layers.{layer}.self_attn.k_proj.bias\", False, 1),\n",
        "        (f\"decoder.layers.{layer}.self_attn.k_proj.weight\", False, 2),\n",
        "        (f\"decoder.layers.{layer}.self_attn.out_proj.bias\", True, None),\n",
        "        (f\"decoder.layers.{layer}.self_attn.out_proj.weight\", False, 1),\n",
        "        (f\"decoder.layers.{layer}.fc1.bias\", False, 1),\n",
        "        (f\"decoder.layers.{layer}.fc1.weight\", False, 2),\n",
        "        (f\"decoder.layers.{layer}.fc2.bias\", True, None),\n",
        "        (f\"decoder.layers.{layer}.fc2.weight\", False, 1),\n",
        "        (f\"decoder.layers.{layer}.self_attn_layer_norm.bias\", False, None),\n",
        "        (f\"decoder.layers.{layer}.self_attn_layer_norm.weight\", False, None),\n",
        "        (f\"decoder.layers.{layer}.final_layer_norm.bias\", False, None),\n",
        "        (f\"decoder.layers.{layer}.final_layer_norm.weight\", False, None),\n",
        "    ])\n",
        "transforms.extend([\n",
        "    (\"decoder.layer_norm.bias\", False, None),\n",
        "    (\"decoder.layer_norm.weight\", False, None),\n",
        "])\n",
        "\n",
        "for i in range(len(transforms)):\n",
        "    transform = transforms.pop(0)\n",
        "\n",
        "    params = torch_checkpoint[\"model\"][transform[0]]\n",
        "\n",
        "    # Need to unscramble fairseq-style embedding matrices\n",
        "    if transform[0] in (\"decoder.embed_tokens.weight\", \"decoder.output_projection.weight\"):\n",
        "        params = params[mapping]\n",
        "        params = torch.cat((params, torch.zeros(padding_rows, params.shape[-1], device=params.device)), dim=0)\n",
        "\n",
        "    # torch.nn.Linear uses a transposed version of the equivalent tensor that\n",
        "    # haiku.Linear uses, so we have to un-transpose the tensor first\n",
        "    if not any(s in transform[0] for s in (\"decoder.embed_tokens.weight\",)):\n",
        "        params = params.T\n",
        "\n",
        "    if transform[2] is not None:\n",
        "        old_shape = (total_shards,) + get_old_shape(params, transform[2])\n",
        "    else:\n",
        "        old_shape = (total_shards, params.shape[0],)\n",
        "    print(f\"< [{transform[0]}] {params.shape} to {old_shape}\")\n",
        "\n",
        "    params = np.asarray(params[None], dtype=jnp.bfloat16)\n",
        "    params = reshard_reverse(params, old_shape, is_shard_bias=transform[1])\n",
        "\n",
        "    if np.isnan(params).any() or np.isinf(params).any():\n",
        "        raise ValueError(f\"bfloat16 overflow/underflow\")\n",
        "\n",
        "    print(f\"> [{transform[0]}] {params.shape}\")\n",
        "    assert params.shape == old_shape\n",
        "    checkpoint.append(params)\n",
        "\n",
        "# Append the checkpoint step number (can be set to an arbitrary value, in this\n",
        "# case 0, as long as we're only using inference and not training the model)\n",
        "checkpoint.append(np.zeros(total_shards, dtype=np.int32))\n",
        "\n",
        "print(\"saving\")\n",
        "save(checkpoint)\n",
        "del checkpoint\n",
        "with open(\"jax_checkpoint/config.json\", \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print(colored(\"DONE! The JAX checkpoint is now stored at ./jax_checkpoint\", \"green\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
